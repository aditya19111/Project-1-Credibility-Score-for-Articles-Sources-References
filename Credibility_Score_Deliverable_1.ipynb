{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverable 1\n",
        "Student name = Aditya Bhavsar\n"
      ],
      "metadata": {
        "id": "UXuejEsrKT6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Required installation of the following library\n",
        "!pip install tldextract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMTcFWzgoIeY",
        "outputId": "67c5a82e-cbc6-45a4-8a90-5494f4e49e07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2025.1.31)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tldextract\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import openai  # For AI-based fact-checking\n",
        "\n",
        "# === SERPAPI & OpenAI API Keys ===\n",
        "SERP_API_KEY = \"Your api key here\" # Replace with your OpenAI API Key\n",
        "OPENAI_API_KEY = \"Your api key here\"  # Replace with your OpenAI API Key\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# === 1st metric - Domain Trust ===\n",
        "def get_domain_trust(url):\n",
        "    \"\"\"Estimates domain trust using search ranking, WHOIS data, and Tranco list.\"\"\"\n",
        "    domain = tldextract.extract(url).registered_domain\n",
        "\n",
        "    # === Check Tranco List ===\n",
        "    try:\n",
        "        tranco_response = requests.get(\"https://tranco-list.eu/top-1m.csv\").text\n",
        "        if domain in tranco_response:\n",
        "            tranco_rank = tranco_response.split(domain)[0].strip().split(\"\\n\")[-1]\n",
        "            tranco_score = max(100 - (int(tranco_rank) / 10000), 50)\n",
        "        else:\n",
        "            tranco_score = 40\n",
        "    except:\n",
        "        tranco_score = 40\n",
        "\n",
        "    # === WHOIS Lookup for Domain Age ===\n",
        "    try:\n",
        "        whois_response = requests.get(f\"https://api.ip2whois.com/v2?key=demo&domain={domain}\").json()\n",
        "        creation_date = whois_response.get(\"created_date\", \"2000-01-01\")\n",
        "        domain_age = (datetime.now() - datetime.strptime(creation_date, \"%Y-%m-%d\")).days // 365\n",
        "        age_score = min(domain_age * 5, 100)\n",
        "    except:\n",
        "        age_score = 50\n",
        "\n",
        "    # === Extract Backlink Score (Using SerpAPI) ===\n",
        "    try:\n",
        "        search = requests.get(f\"https://serpapi.com/search.json?q={domain}&api_key={SERP_API_KEY}\").json()\n",
        "        backlink_count = len(search.get(\"organic_results\", []))\n",
        "        backlink_score = min(backlink_count * 10, 100)\n",
        "    except:\n",
        "        backlink_score = 50\n",
        "\n",
        "    # === Final Domain Trust Calculation ===\n",
        "    domain_trust = (0.4 * tranco_score) + (0.3 * age_score) + (0.3 * backlink_score)\n",
        "    return round(domain_trust, 2)\n",
        "\n",
        "# === 2nd metric - AI Based fact check score ===\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def get_fact_check_score(text):\n",
        "    \"\"\"Evaluates the factual accuracy of a given claim using multiple layers of verification.\"\"\"\n",
        "\n",
        "    # === Step 1: Verify with Search Engine Results ===\n",
        "    try:\n",
        "        serpapi_key = \"78d578eb49c83a63f5945da59e8ae9e37f865cc14da2c714ccb9e990a195d787\"\n",
        "        params = {\n",
        "            \"q\": f\"fact check {text}\",\n",
        "            \"engine\": \"google\",\n",
        "            \"api_key\": serpapi_key\n",
        "        }\n",
        "        response = requests.get(\"https://serpapi.com/search\", params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        # Check for fact-checking websites in top results\n",
        "        trusted_sources = [\"snopes.com\", \"politifact.com\", \"factcheck.org\", \"bbc.com\", \"reuters.com\"]\n",
        "        source_mentions = sum(1 for result in data.get(\"organic_results\", []) if any(domain in result.get(\"link\", \"\") for domain in trusted_sources))\n",
        "\n",
        "        fact_check_score = min(source_mentions * 20, 100)  # Normalize (max 5 sources = 100)\n",
        "    except Exception:\n",
        "        fact_check_score = 50  # Default neutral score\n",
        "\n",
        "    # === Step 2: Cross-Check with Wikipedia ===\n",
        "    try:\n",
        "        wiki_response = requests.get(f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch={text}\")\n",
        "        wiki_data = wiki_response.json()\n",
        "        wiki_matches = len(wiki_data.get(\"query\", {}).get(\"search\", []))\n",
        "\n",
        "        fact_check_score += min(wiki_matches * 10, 30)  # Add extra points if Wikipedia has related articles\n",
        "    except Exception:\n",
        "        pass  # Ignore errors, keep previous score\n",
        "\n",
        "    # === Step 3: NLP Semantic Similarity ===\n",
        "    try:\n",
        "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "        trusted_texts = \" \".join([result[\"title\"] + result[\"snippet\"] for result in data.get(\"organic_results\", []) if \"title\" in result and \"snippet\" in result])\n",
        "\n",
        "        similarity_score = util.pytorch_cos_sim(model.encode(text), model.encode(trusted_texts)).item() * 100\n",
        "        fact_check_score += min(similarity_score / 2, 30)  # Weight this factor to prevent over-scaling\n",
        "    except Exception:\n",
        "        pass  # Ignore errors, keep previous score\n",
        "\n",
        "    # === Final Normalization ===\n",
        "    return max(0, min(fact_check_score, 100))  # Ensure within range\n",
        "\n",
        "# === 3rd metric - Bias Score ===\n",
        "def get_bias_score(text, domain):\n",
        "    \"\"\"Calculates bias score based on sentiment analysis and media bias ratings.\"\"\"\n",
        "    sentiment_pipeline = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    sentiment_result = sentiment_pipeline(text[:512])[0]\n",
        "\n",
        "    # Map sentiment to bias score\n",
        "    sentiment_bias = {\n",
        "        \"1 star\": 30,  # Strongly Negative (Potentially biased)\n",
        "        \"2 stars\": 50,  # Slightly Negative (Neutral to biased)\n",
        "        \"3 stars\": 70,  # Neutral\n",
        "        \"4 stars\": 80,  # Slightly Positive (Neutral to trusted)\n",
        "        \"5 stars\": 100  # Strongly Positive (Trusted)\n",
        "    }\n",
        "    bias_score = sentiment_bias.get(sentiment_result[\"label\"], 50)\n",
        "\n",
        "    # === Fetch Bias Rating from AllSides API ===\n",
        "    try:\n",
        "        bias_response = requests.get(f\"https://api.allsides.com/bias/{domain}\").json()\n",
        "        media_bias = bias_response.get(\"bias\", \"center\")\n",
        "        bias_adjustment = {\n",
        "            \"left\": -20,\n",
        "            \"lean left\": -10,\n",
        "            \"center\": 0,\n",
        "            \"lean right\": 10,\n",
        "            \"right\": 20\n",
        "        }.get(media_bias.lower(), 0)\n",
        "        bias_score = max(0, min(bias_score + bias_adjustment, 100))\n",
        "    except:\n",
        "        pass  # If API fails, use sentiment score only\n",
        "\n",
        "    return round(bias_score, 2)\n",
        "\n",
        "# === 4th metric - Content Relevance Score ===\n",
        "def compute_content_relevance(user_query, page_text):\n",
        "    \"\"\"Computes content relevance using semantic similarity.\"\"\"\n",
        "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "    similarity_score = util.pytorch_cos_sim(model.encode(user_query), model.encode(page_text)).item() * 100\n",
        "    return round(similarity_score, 2)\n",
        "\n",
        "# === 5th metric - Citation Score ===\n",
        "def check_google_scholar(url):\n",
        "    \"\"\"\n",
        "    Computes Citation Score by:\n",
        "    1) Checking backlinks using Google Search (SerpAPI).\n",
        "    2) Searching for academic mentions (PDFs, research papers).\n",
        "    3) Normalizing results to a 0-100 scale.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # === Step 1: Count Backlinks from Google Search ===\n",
        "        backlink_params = {\n",
        "            \"q\": f\"link:{url}\",\n",
        "            \"engine\": \"google\",\n",
        "            \"api_key\": SERP_API_KEY\n",
        "        }\n",
        "        backlink_response = requests.get(\"https://serpapi.com/search\", params=backlink_params).json()\n",
        "        backlink_count = len(backlink_response.get(\"organic_results\", []))\n",
        "\n",
        "        # === Step 2: Count Academic Mentions (PDFs & Research Papers) ===\n",
        "        academic_params = {\n",
        "            \"q\": f\"\\\"{url}\\\" filetype:pdf OR site:researchgate.net OR site:arxiv.org OR site:semanticscholar.org\",\n",
        "            \"engine\": \"google\",\n",
        "            \"api_key\": SERP_API_KEY\n",
        "        }\n",
        "        academic_response = requests.get(\"https://serpapi.com/search\", params=academic_params).json()\n",
        "        academic_count = len(academic_response.get(\"organic_results\", []))\n",
        "\n",
        "        # === Step 3: Normalize Score to 0-100 ===\n",
        "        citation_score = min((backlink_count * 5) + (academic_count * 15), 100)  # Adjust weights if needed\n",
        "\n",
        "        return round(citation_score, 2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching citation data: {e}\")\n",
        "        return 0  # Default if API fails\n",
        "\n",
        "# === Final Step - Final score evaluation method ===\n",
        "def rate_url_validity(user_query, url):\n",
        "    \"\"\"\n",
        "    Evaluates the validity of a given URL by computing multiple credibility metrics.\n",
        "    Returns a final credibility score (0-100).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        page_text = \" \".join([p.text for p in soup.find_all(\"p\")])  # Extract paragraph text\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to fetch content: {str(e)}\"}\n",
        "\n",
        "    domain = tldextract.extract(url).registered_domain\n",
        "\n",
        "    # Compute all credibility scores\n",
        "    domain_trust = get_domain_trust(url)\n",
        "    content_relevance = compute_content_relevance(user_query, page_text)\n",
        "    fact_check_score = get_fact_check_score(page_text)\n",
        "    bias_score = get_bias_score(page_text, domain)\n",
        "    citation_score = check_google_scholar(url)\n",
        "\n",
        "    # === Compute Final Validity Score ===\n",
        "    final_score = (\n",
        "        (0.10 * domain_trust) +\n",
        "        (0.60 * content_relevance) +\n",
        "        (0.15 * fact_check_score) +\n",
        "        (0.15 * bias_score) +\n",
        "        (0.10 * citation_score)\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Domain Trust\": domain_trust,\n",
        "        \"Content Relevance\": content_relevance,\n",
        "        \"Fact-Check Score\": fact_check_score,\n",
        "        \"Bias Score\": bias_score,\n",
        "        \"Citation Score\": citation_score,\n",
        "        \"Final Validity Score\": round(final_score, 2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "gEfGSE6hqvGr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === TESTING ===\n",
        "test_urls = [\n",
        "    \"https://www.bbc.com/news\",\n",
        "    \"https://www.nasa.gov\",\n",
        "    \"https://www.factcheck.org\"\n",
        "]\n",
        "\n",
        "for url in test_urls:\n",
        "    print(f\"\\nResults for {url}:\")\n",
        "    print(rate_url_validity(\"was moon landing by NASA fake!\", url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcvP8vL8w3d-",
        "outputId": "4d4234d2-a105-417f-dd5e-d4c71a50eb51"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for https://www.bbc.com/news:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 61.0, 'Content Relevance': 3.29, 'Fact-Check Score': 6.9031402468681335, 'Bias Score': 80, 'Citation Score': 100, 'Final Validity Score': 31.11}\n",
            "\n",
            "Results for https://www.nasa.gov:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 58.0, 'Content Relevance': 27.55, 'Fact-Check Score': 30, 'Bias Score': 80, 'Citation Score': 100, 'Final Validity Score': 48.83}\n",
            "\n",
            "Results for https://www.factcheck.org:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 64.0, 'Content Relevance': 17.99, 'Fact-Check Score': 5.511181801557541, 'Bias Score': 30, 'Citation Score': 0, 'Final Validity Score': 22.52}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === TESTING ===\n",
        "test_urls_2 = [\n",
        "    \"https://www.pcgamer.com/hardware/graphics-cards/is-the-new-rtx-5070-really-as-fast-as-nvidias-previous-flagship-rtx-4090-gpu-turns-out-the-answer-is-yes-kinda/\",\n",
        "    \"https://www.fool.com/investing/2025/02/09/is-nvidia-still-a-millionaire-maker-stock/\",\n",
        "    \"https://www.tomshardware.com/pc-components/gpus/nvidias-geforce-rtx-5070-at-usd549-how-does-it-stack-up-to-the-previous-generation-rtx-4070\"\n",
        "]\n",
        "\n",
        "for url in test_urls_2:\n",
        "    print(f\"\\nResults for {url}:\")\n",
        "    print(rate_url_validity(\"Nvidia's new RTX 5070 is it really good ?\", url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52HHZlU4DMbB",
        "outputId": "a9a4b5c1-a51b-4c07-f370-18e34a027a9a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for https://www.pcgamer.com/hardware/graphics-cards/is-the-new-rtx-5070-really-as-fast-as-nvidias-previous-flagship-rtx-4090-gpu-turns-out-the-answer-is-yes-kinda/:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 46.0, 'Content Relevance': 65.21, 'Fact-Check Score': 5.508620664477348, 'Bias Score': 50, 'Citation Score': 0, 'Final Validity Score': 52.05}\n",
            "\n",
            "Results for https://www.fool.com/investing/2025/02/09/is-nvidia-still-a-millionaire-maker-stock/:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 46.0, 'Content Relevance': 30.44, 'Fact-Check Score': 5.225852131843567, 'Bias Score': 100, 'Citation Score': 0, 'Final Validity Score': 38.65}\n",
            "\n",
            "Results for https://www.tomshardware.com/pc-components/gpus/nvidias-geforce-rtx-5070-at-usd549-how-does-it-stack-up-to-the-previous-generation-rtx-4070:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 46.0, 'Content Relevance': 65.95, 'Fact-Check Score': 0, 'Bias Score': 70, 'Citation Score': 0, 'Final Validity Score': 54.67}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === TESTING ===\n",
        "test_urls_3 = [\n",
        "    \"https://www.rpgsite.net/review/16825-kingdom-come-deliverance-ii-review\",\n",
        "    \"https://www.pcgamer.com/games/rpg/20-hours-in-kingdom-come-deliverance-2-is-a-mad-systems-driven-sandbox-that-captures-some-of-the-best-parts-of-games-like-stalker/\",\n",
        "    \"https://www.kingdomcomehome.shop/\"\n",
        "]\n",
        "\n",
        "for url in test_urls_3:\n",
        "    print(f\"\\nResults for {url}:\")\n",
        "    print(rate_url_validity(\"kingdome come deliverance 2 got good rating compare to one\", url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVDwvDnuGpGE",
        "outputId": "72ebd32d-65b6-412c-dd3b-9e11c33b802a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for https://www.rpgsite.net/review/16825-kingdom-come-deliverance-ii-review:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 46.0, 'Content Relevance': 59.95, 'Fact-Check Score': 50, 'Bias Score': 70, 'Citation Score': 0, 'Final Validity Score': 58.57}\n",
            "\n",
            "Results for https://www.pcgamer.com/games/rpg/20-hours-in-kingdom-come-deliverance-2-is-a-mad-systems-driven-sandbox-that-captures-some-of-the-best-parts-of-games-like-stalker/:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 46.0, 'Content Relevance': 41.16, 'Fact-Check Score': 8.470191061496735, 'Bias Score': 100, 'Citation Score': 0, 'Final Validity Score': 45.57}\n",
            "\n",
            "Results for https://www.kingdomcomehome.shop/:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 46.0, 'Content Relevance': 14.35, 'Fact-Check Score': 40.282234847545624, 'Bias Score': 100, 'Citation Score': 0, 'Final Validity Score': 34.25}\n"
          ]
        }
      ]
    }
  ]
}